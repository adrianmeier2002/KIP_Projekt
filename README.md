# KIP_Projekt - Minesweeper mit Hybrid AI (Constraint Solver + Reinforcement Learning)

Ein vollstÃ¤ndiges Minesweeper-Spiel mit GUI (PySide6) und **Hybrid AI**: Constraint Solver kombiniert mit Deep Q-Network (DQN).

## ğŸ¯ ProjektÃ¼bersicht

Dieses Projekt implementiert:
- **Minesweeper-Spiel** mit konfigurierbaren Spielfeldern
- **GUI** mit PySide6 fÃ¼r manuelles Spielen
- **Hybrid AI** = **Constraint Solver (100% sichere ZÃ¼ge) + Reinforcement Learning (fÃ¼r Guess-Situationen)**
- **3 Schwierigkeitsgrade**: Leicht (~10%), Mittel (~15%), Schwer (~20%)

## ğŸš€ Warum Hybrid AI?

### **Das Problem mit reinem Reinforcement Learning:**

Unsere Tests zeigten:
- **Reines DQN:** 0% Win-Rate nach 5000 Episoden auf 7x7 Board
- **Problem:** Minesweeper erfordert logische Inferenz, die CNNs nicht gut lernen kÃ¶nnen
- **LÃ¶sung:** **Hybrid-Ansatz!**

### **Hybrid AI = Best of Both Worlds:**

```
1. Constraint Solver findet 100% sichere ZÃ¼ge
   â†’ Nutzt Minesweeper-Logik (z.B. "Zelle zeigt 1, 1 Flagge â†’ Rest ist sicher")
   
2. RL nur fÃ¼r Guess-Situationen
   â†’ Wenn keine sicheren ZÃ¼ge verfÃ¼gbar, wÃ¤hlt RL den "besten" Guess
   
3. Ergebnis:
   âœ… Agent macht keine vermeidbaren Fehler
   âœ… RL lernt nur fÃ¼r schwierige Guess-Situationen
   âœ… Deutlich hÃ¶here Win-Rate!
```

## ğŸ“Š Erwartete Performance

### **Mit Hybrid Agent:**

| Board Size | Difficulty | Expected Win-Rate | Training Time |
|------------|------------|-------------------|---------------|
| 5x5 | Easy | 40-70% | 500-1000 Episodes |
| 7x7 | Medium | 20-50% | 1000-2000 Episodes |
| 9x9 | Medium | 15-35% | 2000-3000 Episodes |

**Wichtig:** Der Solver allein kann bereits ~30-60% lÃ¶sen (je nach Schwierigkeit)!

### **Ohne Hybrid (Pure RL):**

| Board Size | Difficulty | Win-Rate | Problem |
|------------|------------|----------|---------|
| 5x5+ | Any | ~0% | Lernt nicht effektiv |

**â†’ Hybr

id-Modus ist STANDARD und EMPFOHLEN!**

---

## ğŸ›  Installation

### Voraussetzungen

Python 3.8+ muss installiert sein.

### Projekt-Setup

```bash
# 1. Repository klonen
git clone https://github.com/your-repo/KIP_Projekt.git
cd KIP_Projekt

# 2. AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# FÃ¼r Windows:
python -m pip install -r requirements.txt
```

**Windows:** Falls PyTorch-Fehler auftreten, installieren Sie [Visual C++ Redistributable](https://aka.ms/vs/17/release/vc_redist.x64.exe).

---

## ğŸ® Verwendung

### **Option 1: GUI (Manuelles Spielen + Training)**

```bash
python main.py
```

**Features:**
- Linksklick: Zelle aufdecken
- Rechtsklick: Flagge setzen/entfernen
- MenÃ¼ â†’ RL Training starten (mit Visualisierung)
- MenÃ¼ â†’ Modell laden und testen

### **Option 2: Kommandozeile Training (Hybrid Mode - EMPFOHLEN)**

```bash
python -m src.reinforcement_learning.trainer \
  --episodes 1000 \
  --difficulty medium \
  --width 7 --height 7 \
  --save-path models/hybrid_7x7.pth \
  --eval-episodes 25
```

**Wichtig:** Hybrid-Modus ist **standardmÃ¤ÃŸig aktiviert**!

**Parameter:**
- `--episodes`: Anzahl Trainingsepisoden (Standard: 1000)
- `--difficulty`: easy, medium, hard (Standard: medium)
- `--width / --height`: BrettgrÃ¶ÃŸe (Optional)
- `--save-path`: Modell-Speicherpfad
- `--eval-episodes`: Evaluations-Episoden pro Log (zeigt echte Win-Rate)
- `--no-hybrid`: **Deaktiviert Hybrid-Modus** (nicht empfohlen!)

### **Option 3: Pure RL (Zum Vergleich - NICHT EMPFOHLEN)**

```bash
python -m src.reinforcement_learning.trainer \
  --episodes 5000 \
  --difficulty easy \
  --width 5 --height 5 \
  --no-hybrid \
  --save-path models/pure_rl_5x5.pth
```

**Erwartung:** Win-Rate bleibt bei ~0%. Nur fÃ¼r Benchmarking!

---

## ğŸ“ˆ Training verstehen

### **Wichtige Metriken:**

```
Episode 100/1000
  Avg Reward: 45.23
  Avg Length: 12.4          â† ZÃ¼ge pro Episode (hÃ¶her = besser)
  Win Rate: 15.0%           â† Training Win-Rate (mit Exploration)
  Epsilon: 0.825
  Eval (Îµ=0) â†’ Win Rate: 35.0% | Avg Len: 15.2    â† ECHTE Win-Rate!
  Solver Usage â†’ 65.3% | RL Guesses: 34.7%        â† Hybrid-Statistik
```

**Was die Zahlen bedeuten:**

- **Avg Length:** Wie lange Ã¼berlebt der Agent?
  - Zu niedrig (<5 auf 7x7) = stirbt zu frÃ¼h
  - Gut (>10 auf 7x7) = macht Fortschritt

- **Win Rate (Training):** Mit Exploration (epsilon)
  - Oft niedriger wegen ZufallszÃ¼gen

- **Eval Win Rate (Îµ=0):** **WICHTIGSTE METRIK!**
  - Ohne Exploration = echte Policy-QualitÃ¤t
  - Sollte kontinuierlich steigen

- **Solver Usage:** % der ZÃ¼ge vom Constraint Solver
  - 60-80% = gut (viele sichere ZÃ¼ge genutzt)
  - <30% = Problem (wenige sichere Situationen)

### **Lern-Verlauf (typisch):**

```
Episodes 1-200:    Win-Rate: 10-20% (Exploration)
Episodes 200-500:  Win-Rate: 20-40% (Lernt Guess-Strategie)
Episodes 500-1000: Win-Rate: 30-60% (Konvergenz)
```

**Wenn Win-Rate bei 0% stagniert:**
- ÃœberprÃ¼fen Sie: Ist Hybrid-Modus aktiv? (`--no-hybrid` NICHT verwenden)
- Reduzieren Sie Schwierigkeit (easy statt medium)
- Reduzieren Sie BrettgrÃ¶ÃŸe (5x5 statt 7x7)

---

## ğŸ— Architektur

### **Komponenten:**

```
src/
â”œâ”€â”€ minesweeper/              # Spiel-Logik
â”‚   â”œâ”€â”€ board.py              # Spielfeld-Verwaltung
â”‚   â”œâ”€â”€ cell.py               # Zellen-Logik
â”‚   â””â”€â”€ game.py               # Spiel-Steuerung
â”‚
â”œâ”€â”€ gui/                      # GUI (PySide6)
â”‚   â”œâ”€â”€ game_board.py
â”‚   â”œâ”€â”€ main_window.py
â”‚   â””â”€â”€ rl_visualizer.py
â”‚
â””â”€â”€ reinforcement_learning/   # AI
    â”œâ”€â”€ constraint_solver.py  # âœ¨ Findet 100% sichere ZÃ¼ge
    â”œâ”€â”€ hybrid_agent.py       # âœ¨ Kombiniert Solver + RL
    â”œâ”€â”€ dqn_agent.py          # Deep Q-Network (RL-Teil)
    â”œâ”€â”€ network.py            # CNN-Architektur
    â”œâ”€â”€ environment.py        # RL-Environment
    â””â”€â”€ trainer.py            # Training-Loop
```

### **Hybrid Agent - Funktionsweise:**

```python
def select_action(state, game):
    # 1. Versuche Constraint Solver
    safe_moves = solver.get_safe_moves(game)
    if safe_moves:
        return random.choice(safe_moves)  # 100% sicher!
    
    # 2. Kein sicherer Zug â†’ RL wÃ¤hlt "besten" Guess
    return dqn_agent.select_action(state)
```

### **State Representation (9 KanÃ¤le):**

```
Channel 0: Basis-Encoding (-0.8 hidden, -0.5 flag, -1 mine, 0-1 number)
Channel 1: Hidden-Maske
Channel 2: Flag-Maske
Channel 3: Aufgedeckte Zahlen
Channel 4: Verdeckte Nachbarn
Channel 5: Geflaggte Nachbarn
Channel 6: Hinweis-Summe
Channel 7: Frontier-Maske (neben aufgedeckten Zellen)
Channel 8: Safe-Cell-Maske (100% sichere Zellen)
```

### **Network Architecture:**

```
Input: (9, H, W)
Conv Stack (4 Ã— 128 filters) + BatchNorm + ReLU
AdaptiveAvgPool2d(8Ã—8)
FC: 8192 â†’ 512 â†’ 512 â†’ num_actions
```

---

## ğŸ§ª Tests

```bash
# Alle Tests
python -m pytest tests/ -v

# Nur RL-Tests
python -m pytest tests/reinforcement_learning/ -v

# Nur Minesweeper-Tests
python -m pytest tests/minesweeper/ -v
```

---

## ğŸ“š Dokumentation

- **[RL_IMPLEMENTATION_GUIDE.md](docs/RL_IMPLEMENTATION_GUIDE.md)**: Technische Details
- **[RL_TRAINING_GUIDE.md](docs/RL_TRAINING_GUIDE.md)**: Training-Anleitung
- **[CHANGELOG_RL_FIX_V3.md](CHANGELOG_RL_FIX_V3.md)**: Versionshistorie

---

## ğŸ“ Lessons Learned

### **Warum Reines RL scheiterte:**

1. **Sparse Rewards:** Agent stirbt in 95% der FÃ¤lle sofort
2. **Kombinatorische Explosion:** Zu viele ZustÃ¤nde
3. **Logische Inferenz:** CNNs kÃ¶nnen Minesweeper-Regeln nicht effektiv lernen
4. **Sample-Ineffizienz:** Braucht Millionen statt Tausende Episoden

### **Warum Hybrid funktioniert:**

1. **Sichere ZÃ¼ge garantiert:** Solver macht keine Fehler
2. **RL nur fÃ¼r Guesses:** Fokussiert auf schwierige Situationen
3. **DomÃ¤nenwissen:** Minesweeper-Logik integriert
4. **Sample-Effizienz:** Lernt schneller durch weniger Fehler

### **Generelle Erkenntnis:**

**FÃ¼r Constraint-basierte Probleme:**
- Hybrid-AnsÃ¤tze (Rule-Based + ML) > Reines ML
- DomÃ¤nenwissen beschleunigt Lernen massiv
- CNNs sind schlecht in kombinatorischer Logik

---

## ğŸ”¬ WeiterfÃ¼hrende Experimente

### **Experiment 1: Solver vs. RL Performance**

```bash
# Pure Solver (keine RL)
# â†’ Baseline Win-Rate messen

# Pure RL (--no-hybrid)
# â†’ Zeigt RL-Limitation

# Hybrid
# â†’ Beste Performance
```

### **Experiment 2: Schwierigkeitsgrade**

```bash
# Easy: 10% Minen â†’ 50-70% Win-Rate erwartet
# Medium: 15% Minen â†’ 30-50% Win-Rate erwartet  
# Hard: 20% Minen â†’ 15-35% Win-Rate erwartet
```

### **Experiment 3: BrettgrÃ¶ÃŸen**

```bash
# 5x5: Schnelles Training, hÃ¶here Win-Rate
# 7x7: Moderates Training, mittlere Win-Rate
# 9x9+: Langsames Training, niedrige Win-Rate
```

---

## ğŸ¤ Mitwirkende

Projekt entwickelt mit KI-Assistenz (Cursor + Claude Sonnet) zur Evaluation von AI-gestÃ¼tzter Programmierung.

---

## ğŸ“„ Lizenz

[Lizenz hier einfÃ¼gen]

---

## ğŸš€ Quick Start fÃ¼r Ungeduldige

```bash
# Installation
pip install -r requirements.txt

# Training starten (Hybrid Mode, 7x7, Medium)
python -m src.reinforcement_learning.trainer \
  --episodes 1500 \
  --difficulty medium \
  --width 7 --height 7 \
  --save-path models/hybrid_7x7.pth

# Erwartung: Win-Rate steigt auf 25-45%!
```

**Viel Erfolg! ğŸ‰**

