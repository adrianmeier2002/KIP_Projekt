# KIP_Projekt - Minesweeper mit Hybrid AI (Constraint Solver + Reinforcement Learning)

Ein vollst√§ndiges Minesweeper-Spiel mit GUI (PySide6) und **Hybrid AI**: Constraint Solver kombiniert mit Deep Q-Network (DQN).

## üéØ Projekt√ºbersicht

Dieses Projekt implementiert:
- **Minesweeper-Spiel** mit konfigurierbaren Spielfeldern
- **GUI** mit PySide6 f√ºr manuelles Spielen
- **Hybrid AI** = **Constraint Solver (100% sichere Z√ºge) + Reinforcement Learning (f√ºr Guess-Situationen)**
- **3 Schwierigkeitsgrade**: Leicht (~10%), Mittel (~15%), Schwer (~20%)

## üöÄ Warum Hybrid AI?

### **Das Problem mit reinem Reinforcement Learning:**

Unsere Tests zeigten:
- **Reines DQN:** 0% Win-Rate nach 5000 Episoden auf 7x7 Board
- **Problem:** Minesweeper erfordert logische Inferenz, die CNNs nicht gut lernen k√∂nnen
- **L√∂sung:** **Hybrid-Ansatz!**

### **Hybrid AI = Best of Both Worlds:**

```
1. Constraint Solver findet 100% sichere Z√ºge
   ‚Üí Nutzt Minesweeper-Logik (z.B. "Zelle zeigt 1, 1 Flagge ‚Üí Rest ist sicher")
   
2. RL nur f√ºr Guess-Situationen
   ‚Üí Wenn keine sicheren Z√ºge verf√ºgbar, w√§hlt RL den "besten" Guess
   
3. Ergebnis:
   ‚úÖ Agent macht keine vermeidbaren Fehler
   ‚úÖ RL lernt nur f√ºr schwierige Guess-Situationen
   ‚úÖ Deutlich h√∂here Win-Rate!
```


## üõ† Installation

### Voraussetzungen

- **Python 3.8 oder h√∂her** muss installiert sein
- **Empfohlen:** Python 3.10 oder neuer f√ºr beste Kompatibilit√§t
- **Optional:** CUDA-f√§hige GPU f√ºr schnelleres Training (CPU funktioniert auch)

### Schritt-f√ºr-Schritt Installation

#### 1. Repository klonen oder herunterladen

```bash
# Mit Git:
git clone https://github.com/your-repo/KIP_Projekt.git
cd KIP_Projekt

# Oder: ZIP-Datei herunterladen und entpacken
```

#### 2. Virtuelle Umgebung erstellen (empfohlen)

```bash
# Virtuelle Umgebung erstellen
python -m venv venv

# Aktivieren:
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate
```

#### 3. Abh√§ngigkeiten installieren

```bash
pip install -r requirements.txt

# F√ºr Windows (falls pip nicht funktioniert):
python -m pip install -r requirements.txt
```

#### 4. Installation √ºberpr√ºfen

```bash
# Tests ausf√ºhren um sicherzustellen, dass alles funktioniert:
pytest tests/ -v
```

**Alle 81 Tests sollten bestehen!** ‚úÖ

### M√∂gliche Probleme und L√∂sungen

**Problem:** PyTorch-Fehler unter Windows  
**L√∂sung:** Installieren Sie [Visual C++ Redistributable](https://aka.ms/vs/17/release/vc_redist.x64.exe)

**Problem:** Import-Fehler bei PySide6  
**L√∂sung:** Neuinstallation: `pip install --upgrade --force-reinstall PySide6`

**Problem:** Tests schlagen fehl  
**L√∂sung:** Stellen Sie sicher, dass Sie im Projektverzeichnis sind und alle Dependencies installiert sind

---

## üéÆ Verwendung

### **Option 1: GUI starten (Empfohlen f√ºr Einsteiger)**

```bash
python main.py
```

Das Spiel √∂ffnet sich in einem Fenster. Sie k√∂nnen:

**Spielen:**
- **Linksklick:** Zelle aufdecken
- **Rechtsklick:** Flagge setzen/entfernen
- **Power-Ups nutzen:** Radar (70P), Scanner (70P), Blitz (50P)
- **Neues Spiel:** Men√º ‚Üí Spiel ‚Üí Neues Spiel (Leicht/Mittel/Schwer)
- **Spielfeldgr√∂√üe √§ndern:** Men√º ‚Üí Spiel ‚Üí Spielfeldgr√∂√üe √§ndern

**RL-Training:**
- **Training starten:** Men√º ‚Üí Reinforcement Learning ‚Üí Training starten
- **Modell laden:** Men√º ‚Üí Reinforcement Learning ‚Üí Modell laden und testen
- Der Agent wird dann das Spiel automatisch spielen und Sie k√∂nnen ihm zusehen!

### **Option 2: Kommandozeile Training (Hybrid Mode - EMPFOHLEN)**

```bash
python -m src.reinforcement_learning.trainer \
  --episodes 1000 \
  --difficulty medium \
  --width 7 --height 7 \
  --save-path models/hybrid_7x7.pth \
  --eval-episodes 25
```

**Wichtig:** Hybrid-Modus ist **standardm√§√üig aktiviert**!

**Parameter:**
- `--episodes`: Anzahl Trainingsepisoden (Standard: 1000)
- `--difficulty`: easy, medium, hard (Standard: medium)
- `--width / --height`: Brettgr√∂√üe (Optional)
- `--save-path`: Modell-Speicherpfad
- `--eval-episodes`: Evaluations-Episoden pro Log (zeigt echte Win-Rate)
- `--no-hybrid`: **Deaktiviert Hybrid-Modus** (nicht empfohlen!)

### **Option 3: Pure RL (Zum Vergleich - NICHT EMPFOHLEN)**

```bash
python -m src.reinforcement_learning.trainer \
  --episodes 5000 \
  --difficulty easy \
  --width 5 --height 5 \
  --no-hybrid \
  --save-path models/pure_rl_5x5.pth
```

**Erwartung:** Win-Rate bleibt bei ~0%. Nur f√ºr Benchmarking!

---

## üìà Training verstehen

### **Wichtige Metriken:**

```
Episode 100/1000
  Avg Reward: 45.23
  Avg Length: 12.4          ‚Üê Z√ºge pro Episode (h√∂her = besser)
  Win Rate: 15.0%           ‚Üê Training Win-Rate (mit Exploration)
  Epsilon: 0.825
  Eval (Œµ=0) ‚Üí Win Rate: 35.0% | Avg Len: 15.2    ‚Üê ECHTE Win-Rate!
  Solver Usage ‚Üí 65.3% | RL Guesses: 34.7%        ‚Üê Hybrid-Statistik
```

**Was die Zahlen bedeuten:**

- **Avg Length:** Wie lange √ºberlebt der Agent?
  - Zu niedrig (<5 auf 7x7) = stirbt zu fr√ºh
  - Gut (>10 auf 7x7) = macht Fortschritt

- **Win Rate (Training):** Mit Exploration (epsilon)
  - Oft niedriger wegen Zufallsz√ºgen

- **Eval Win Rate (Œµ=0):** **WICHTIGSTE METRIK!**
  - Ohne Exploration = echte Policy-Qualit√§t
  - Sollte kontinuierlich steigen

- **Solver Usage:** % der Z√ºge vom Constraint Solver
  - 60-80% = gut (viele sichere Z√ºge genutzt)
  - <30% = Problem (wenige sichere Situationen)

### **Lern-Verlauf (typisch):**

```
Episodes 1-200:    Win-Rate: 10-20% (Exploration)
Episodes 200-500:  Win-Rate: 20-40% (Lernt Guess-Strategie)
Episodes 500-1000: Win-Rate: 30-60% (Konvergenz)
```

**Wenn Win-Rate bei 0% stagniert:**
- √úberpr√ºfen Sie: Ist Hybrid-Modus aktiv? (`--no-hybrid` NICHT verwenden)
- Reduzieren Sie Schwierigkeit (easy statt medium)
- Reduzieren Sie Brettgr√∂√üe (5x5 statt 7x7)

---

## üèó Architektur

### **Komponenten:**

```
src/
‚îú‚îÄ‚îÄ minesweeper/              # Spiel-Logik
‚îÇ   ‚îú‚îÄ‚îÄ board.py              # Spielfeld-Verwaltung
‚îÇ   ‚îú‚îÄ‚îÄ cell.py               # Zellen-Logik
‚îÇ   ‚îî‚îÄ‚îÄ game.py               # Spiel-Steuerung
‚îÇ
‚îú‚îÄ‚îÄ gui/                      # GUI (PySide6)
‚îÇ   ‚îú‚îÄ‚îÄ game_board.py
‚îÇ   ‚îú‚îÄ‚îÄ main_window.py
‚îÇ   ‚îî‚îÄ‚îÄ rl_visualizer.py
‚îÇ
‚îî‚îÄ‚îÄ reinforcement_learning/   # AI
    ‚îú‚îÄ‚îÄ constraint_solver.py  # ‚ú® Findet 100% sichere Z√ºge
    ‚îú‚îÄ‚îÄ hybrid_agent.py       # ‚ú® Kombiniert Solver + RL
    ‚îú‚îÄ‚îÄ dqn_agent.py          # Deep Q-Network (RL-Teil)
    ‚îú‚îÄ‚îÄ network.py            # CNN-Architektur
    ‚îú‚îÄ‚îÄ environment.py        # RL-Environment
    ‚îî‚îÄ‚îÄ trainer.py            # Training-Loop
```

### **Hybrid Agent - Funktionsweise:**

```python
def select_action(state, game):
    # 1. Versuche Constraint Solver
    safe_moves = solver.get_safe_moves(game)
    if safe_moves:
        return random.choice(safe_moves)  # 100% sicher!
    
    # 2. Kein sicherer Zug ‚Üí RL w√§hlt "besten" Guess
    return dqn_agent.select_action(state)
```

### **State Representation (9 Kan√§le):**

```
Channel 0: Basis-Encoding (-0.8 hidden, -0.5 flag, -1 mine, 0-1 number)
Channel 1: Hidden-Maske
Channel 2: Flag-Maske
Channel 3: Aufgedeckte Zahlen
Channel 4: Verdeckte Nachbarn
Channel 5: Geflaggte Nachbarn
Channel 6: Hinweis-Summe
Channel 7: Frontier-Maske (neben aufgedeckten Zellen)
Channel 8: Safe-Cell-Maske (100% sichere Zellen)
```

### **Network Architecture:**

```
Input: (9, H, W)
Conv Stack (4 √ó 128 filters) + BatchNorm + ReLU
AdaptiveAvgPool2d(8√ó8)
FC: 8192 ‚Üí 512 ‚Üí 512 ‚Üí num_actions
```

---

## üß™ Tests

Das Projekt enth√§lt umfassende Tests (81 Tests insgesamt):

```bash
# Alle Tests ausf√ºhren (empfohlen)
pytest tests/ -v

# Oder mit Python-Modul:
python -m pytest tests/ -v

# Nur RL-Tests
pytest tests/reinforcement_learning/ -v

# Nur Minesweeper-Tests  
pytest tests/minesweeper/ -v

# Mit Coverage-Report
pytest tests/ --cov=src --cov-report=html
```

**Alle 81 Tests sollten bestehen!** Wenn nicht, √ºberpr√ºfen Sie Ihre Installation.

---

## üìö Dokumentation

Zus√§tzliche Dokumentation finden Sie in folgenden Dateien:

- **[RL_IMPLEMENTATION_GUIDE.md](docs/RL_IMPLEMENTATION_GUIDE.md)**: Technische Details zur RL-Implementierung
- **[RL_TRAINING_GUIDE.md](docs/RL_TRAINING_GUIDE.md)**: Detaillierte Trainingsanleitung
- **[CHANGELOG_RL_FIX_V3.md](CHANGELOG_RL_FIX_V3.md)**: Versionshistorie und √Ñnderungen

### Projekt-Struktur

```
KIP_Projekt/
‚îú‚îÄ‚îÄ main.py                          # Haupteinstiegspunkt (GUI starten)
‚îú‚îÄ‚îÄ requirements.txt                 # Python-Abh√§ngigkeiten
‚îú‚îÄ‚îÄ README.md                        # Diese Datei
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ minesweeper/                # Spiellogik
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ board.py                # Spielfeld-Verwaltung
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cell.py                 # Zellen-Logik
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ game.py                 # Spiel-Steuerung (inkl. Power-Ups)
‚îÇ   ‚îú‚îÄ‚îÄ gui/                        # Grafische Benutzeroberfl√§che
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main_window.py          # Hauptfenster
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ game_board.py           # Spielfeld-Widget
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ menu_bar.py             # Men√ºleiste
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rl_visualizer.py        # RL-Agent Visualisierung
‚îÇ   ‚îú‚îÄ‚îÄ reinforcement_learning/     # KI-Komponenten
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constraint_solver.py    # ‚ú® Regelbasierter Solver (100% sichere Z√ºge)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_agent.py         # ‚ú® Hybrid-Agent (Solver + RL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py            # Deep Q-Network Agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network.py              # Neuronales Netzwerk
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ environment.py          # RL-Environment Wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py              # Training-Skript
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ constants.py            # Konstanten und Konfiguration
‚îú‚îÄ‚îÄ tests/                          # Unit-Tests (81 Tests)
‚îÇ   ‚îú‚îÄ‚îÄ minesweeper/                # Tests f√ºr Spiellogik
‚îÇ   ‚îî‚îÄ‚îÄ reinforcement_learning/    # Tests f√ºr RL-Komponenten
‚îú‚îÄ‚îÄ models/                         # Trainierte Modelle (wird erstellt)
‚îî‚îÄ‚îÄ docs/                           # Zus√§tzliche Dokumentation
```

---

## üéì Lessons Learned

### **Warum Reines RL scheiterte:**

1. **Sparse Rewards:** Agent stirbt in 95% der F√§lle sofort
2. **Kombinatorische Explosion:** Zu viele Zust√§nde
3. **Logische Inferenz:** CNNs k√∂nnen Minesweeper-Regeln nicht effektiv lernen
4. **Sample-Ineffizienz:** Braucht Millionen statt Tausende Episoden

### **Warum Hybrid funktioniert:**

1. **Sichere Z√ºge garantiert:** Solver macht keine Fehler
2. **RL nur f√ºr Guesses:** Fokussiert auf schwierige Situationen
3. **Dom√§nenwissen:** Minesweeper-Logik integriert
4. **Sample-Effizienz:** Lernt schneller durch weniger Fehler

### **Generelle Erkenntnis:**

**F√ºr Constraint-basierte Probleme:**
- Hybrid-Ans√§tze (Rule-Based + ML) > Reines ML
- Dom√§nenwissen beschleunigt Lernen massiv
- CNNs sind schlecht in kombinatorischer Logik


## üõ† Technologie-Stack

- **Python 3.8+**: Programmiersprache
- **PySide6 6.10+**: GUI Framework (Qt f√ºr Python)
- **PyTorch 2.9+**: Deep Learning Framework
- **NumPy 2.3+**: Numerische Berechnungen
- **Pytest 9.0+**: Testing Framework

## üéÆ Spiel-Features und Herausforderungen

Dieses Minesweeper-Spiel bietet zus√§tzliche Features und Herausforderungen, die das klassische Spiel erweitern:

### **Power-Ups (mit Punktesystem)**

Durch das Aufdecken von Feldern verdienen Sie Punkte (1 Punkt pro Feld). Diese Punkte k√∂nnen Sie f√ºr Power-Ups ausgeben:

#### **üì° Radar (70 Punkte)**
- **Funktion:** Deckt einen 3√ó3-Bereich sofort auf
- **Besonderheit:** Minen in diesem Bereich werden **nicht** ausgel√∂st, sondern nur mit einem Warnsymbol ‚ö†Ô∏è markiert
- **Verwendung:** Klicken Sie auf den Radar-Button und dann auf die gew√ºnschte Zelle als Zentrum
- **Tipp:** Ideal f√ºr gef√§hrliche Bereiche, wo Sie vermuten, dass Minen sein k√∂nnten

#### **üîç Scanner (70 Punkte)**
- **Funktion:** Z√§hlt die Anzahl der Minen in einem 3√ó3-Bereich
- **Anzeige:** Das Ergebnis (z.B. "üîç3") wird auf der gescannten Zelle angezeigt
- **Verwendung:** Klicken Sie auf den Scanner-Button und dann auf die gew√ºnschte Zelle als Zentrum
- **Tipp:** Nutzen Sie dies, um zus√§tzliche Informationen f√ºr Ihre Strategie zu erhalten

#### **‚ö° Blitz (50 Punkte)**
- **Funktion:** Deckt automatisch 1-3 sichere Felder auf
- **Intelligenz:** Nutzt den Constraint-Solver, um nur **garantiert sichere** Felder aufzudecken
- **Verwendung:** Einfach auf den Blitz-Button klicken
- **Tipp:** Perfekt, wenn Sie feststecken und keine sichere Wahl sehen

### **Herausforderungen**

Das Spiel f√ºgt dynamisch Herausforderungen hinzu, um das Spielerlebnis spannender zu gestalten:

#### **‚ùì Mystery-Felder**
- **Was ist das?** Aufgedeckte Zahlenfelder werden zu Fragezeichen ‚ùì
- **Problem:** Sie k√∂nnen die echte Zahl nicht sehen!
- **L√∂sung:** Zahlen Sie 20 Punkte, um die Mystery-Zahl zu enth√ºllen
- **H√§ufigkeit:** Erscheint alle 15-25 aufgedeckten Felder
- **Strategie:** √úberlegen Sie gut, ob Sie die Punkte ausgeben m√∂chten oder ob Sie auch ohne diese Information weiterkommen

#### **‚ö° Speed-Felder**
- **Was ist das?** Ein aufgedecktes Feld startet einen 5-Sekunden-Timer
- **Anzeige:** Gro√üer roter Timer oben im Fenster mit Countdown
- **Ziel:** Decken Sie ein weiteres Feld auf, bevor die Zeit abl√§uft!
- **Konsequenz:** Wenn die Zeit abl√§uft ‚Üí **Game Over**
- **H√§ufigkeit:** Erscheint alle 20-30 aufgedeckten Felder
- **Strategie:** Halten Sie immer sichere Z√ºge bereit f√ºr den Fall, dass ein Speed-Feld erscheint

#### **üéÆ Tetris-Felder**
- **Was ist das?** Ein aufgedecktes Feld aktiviert den Tetris-Modus
- **Anzeige:** Oben erscheint eine Tetris-Form (I, O, T, S, Z, L oder J)
- **Ziel:** Platzieren Sie diese Form auf dem Spielfeld
- **Regel:** Die Form muss vollst√§ndig auf verdeckte, minenfreie Felder passen
- **Vorschau:** Beim Hover √ºber dem Feld sehen Sie, wo die Form platziert w√ºrde
- **Effekt:** Alle Felder der Form werden gleichzeitig aufgedeckt
- **H√§ufigkeit:** Erscheint alle 25-40 aufgedeckten Felder
- **Strategie:** Versuchen Sie, die Form in einem sicheren Bereich zu platzieren

### **Herausforderungen deaktivieren**

F√ºr das RL-Training k√∂nnen Herausforderungen optional deaktiviert werden:
- Im Code: `Game(..., enable_challenges=False)`
- Dies erm√∂glicht fokussiertes Training ohne zuf√§llige Komplikationen

---

## üöÄ Quick Start f√ºr Ungeduldige

```bash
# 1. Installation
pip install -r requirements.txt

# 2. Spiel starten
python main.py

# 3. Optional: Training starten (Hybrid Mode, 7x7, Medium)
python -m src.reinforcement_learning.trainer \
  --episodes 1500 \
  --difficulty medium \
  --width 7 --height 7 \
  --save-path models/hybrid_7x7.pth
```

**Viel Erfolg! üéâ**

