# Implementierungs-Guide: Minesweeper mit RL

Dieses Dokument erklÃ¤rt die Implementierung jedes Teils des Projekts und die Design-Entscheidungen.

## Phase 1: Projekt-Setup und Grundstruktur

### Was wurde gemacht:

1. **Projektstruktur erstellt** - Modulare Struktur:
   - `src/minesweeper/` - Spiellogik (Kern)
   - `src/gui/` - GUI-Komponenten
   - `src/reinforcement_learning/` - RL-Implementierung
   - `src/utils/` - Hilfsfunktionen

2. **Konstanten definiert** (`src/utils/constants.py`):
   - SpielfeldgrÃ¶ÃŸe: 20x30 (600 Zellen)
   - 3 Schwierigkeitsgrade:
     - Leicht: ~10% Minen (~60 Minen)
     - Mittel: ~15% Minen (~90 Minen) - Standard
     - Schwer: ~20% Minen (~120 Minen)
   - ZellzustÃ¤nde: HIDDEN, REVEALED, FLAGGED, MINE

3. **Dependencies** (`requirements.txt`):
   - PySide6: GUI Framework
   - PyTorch: Deep Learning fÃ¼r RL
   - NumPy: Numerische Operationen

---

## Phase 2: Minesweeper Kern-Logik

### 2.1 Cell-Klasse (`src/minesweeper/cell.py`)

**Zweck:** ReprÃ¤sentiert eine einzelne Zelle im Spielfeld.

**Implementierung:**
- **Zustandsverwaltung:** Jede Zelle hat einen Zustand (verdeckt, aufgedeckt, markiert)
- **Minen-Markierung:** `set_mine()` markiert die Zelle als Mine
- **NachbarzÃ¤hlung:** `adjacent_mines` speichert die Anzahl benachbarter Minen
- **Reveal-Logik:** `reveal()` deckt die Zelle auf (nur wenn verdeckt)
- **Flag-Logik:** `flag()` togglet die Flagge (nur wenn verdeckt)

**Design-Entscheidungen:**
- Verwendet Konstanten statt Magic Numbers fÃ¼r bessere Lesbarkeit
- RÃ¼ckgabewerte (`True`/`False`) zeigen an, ob Operation erfolgreich war
- Getter-Methoden (`is_revealed()`, `is_flagged()`, `is_hidden()`) fÃ¼r klare API

**MÃ¶gliche Optimierungen:**
- âœ… Aktuell: Gut strukturiert, keine Optimierungen nÃ¶tig

---

### 2.2 Board-Klasse (`src/minesweeper/board.py`)

**Zweck:** Verwaltet das gesamte Spielfeld und die Minen-Platzierung.

**Implementierung:**
- **Spielfeld-Generation:** Erstellt 2D-Array von Cell-Objekten
- **Minen-Platzierung:** `place_mines()` platziert Minen zufÃ¤llig (ausschlieÃŸlich erster Klick)
- **NachbarzÃ¤hlung:** `_calculate_adjacent_mines()` berechnet fÃ¼r jede Zelle die Anzahl benachbarter Minen
- **Nachbar-Abfrage:** `get_neighbors()` gibt alle 8 Nachbarzellen zurÃ¼ck

**Design-Entscheidungen:**
- **Lazy Mine Placement:** Minen werden erst beim ersten Klick platziert (verhindert sofortigen Verlust)
- **Mine-Positions-Tracking:** `mine_positions` Set speichert alle Minen-Positionen fÃ¼r schnellen Zugriff
- **GrenzenprÃ¼fung:** `get_cell()` prÃ¼ft Array-Grenzen und gibt `None` fÃ¼r ungÃ¼ltige Positionen zurÃ¼ck

**MÃ¶gliche Optimierungen:**
- âœ… Aktuell: Effizient implementiert
- ðŸ’¡ Potenzial: Caching von Nachbarzellen fÃ¼r sehr groÃŸe Spielfelder (aktuell nicht nÃ¶tig bei 20x30)

---

### 2.3 Game-Klasse (`src/minesweeper/game.py`)

**Zweck:** Verwaltet die gesamte Spiellogik und Spielzustand.

**Implementierung:**
- **Spielzustand:** PLAYING, WON, LOST
- **Erster Klick:** Triggert Minen-Platzierung (ausschlieÃŸlich geklickter Zelle)
- **Aufdecken:** `reveal_cell()` prÃ¼ft auf Mine, deckt auf, prÃ¼ft Gewinn
- **Auto-Aufdecken:** `_auto_reveal_safe_neighbors()` deckt automatisch sichere Nachbarn auf (BFS-Algorithmus)
- **Flaggen:** `toggle_flag()` setzt/entfernt Flaggen
- **Schwierigkeitsgrade:** Dynamische Minen-Anzahl basierend auf Schwierigkeit

**Design-Entscheidungen:**
- **Auto-Aufdecken:** BFS (Breadth-First Search) fÃ¼r effizientes Aufdecken von Bereichen mit 0 Minen
- **Erster Klick:** Garantiert, dass erste Zelle sicher ist (keine Mine)
- **State Management:** Klare Zustandsverwaltung mit `GameState` Enumeration

**MÃ¶gliche Optimierungen:**
- âœ… Aktuell: Gut implementiert
- ðŸ’¡ Potenzial: 
  - Timer-Integration (bereits in GUI vorhanden)
  - Highscore-System
  - Hint-System fÃ¼r schwierige Situationen

---

## Phase 3: GUI Implementation

### 3.1 GameBoard Widget (`src/gui/game_board.py`)

**Zweck:** Zeigt das Spielfeld als interaktive Buttons an.

**Implementierung:**
- **Custom Button:** `CellButton` erbt von `QPushButton` mit Signal-System
- **Grid-Layout:** 20x30 Grid von Buttons
- **Interaktion:** Linksklick = Aufdecken, Rechtsklick = Flagge
- **Visualisierung:** Farbcodierung fÃ¼r Zahlen, Icons fÃ¼r Minen/Flaggen

**Design-Entscheidungen:**
- **Signals:** Verwendet PySide6 Signals fÃ¼r lose Kopplung
- **Update-Mechanismus:** `_update_display()` aktualisiert alle Buttons basierend auf Spielzustand

---

### 3.2 MainWindow (`src/gui/main_window.py`)

**Zweck:** Hauptfenster der Anwendung.

**Implementierung:**
- **Menu-Bar:** Schwierigkeitsgrade, Neues Spiel
- **Status-Bar:** Minen-ZÃ¤hler, Timer
- **Game-Board:** Integriertes Spielfeld
- **Event-Handling:** Gewinn/Verlust-Meldungen

---

## Phase 4: Reinforcement Learning

### 4.1 Environment (`src/reinforcement_learning/environment.py`)

**Zweck:** Gym-Ã¤hnlicher Wrapper rund um die Minesweeper-Logik.

**Implementierung (aktuelle Version):**
- **State Representation:** 7 KanÃ¤le (Hidden-, Flag-, Zahlenmasken, Nachbarschaftsdichten und Hinweissumme). Alle Werte liegen in `[-1, 1]`.
- **Action Space:** StandardmÃ¤ÃŸig `width Ã— height` (Reveal-only, inspiriert durch [sdlee94](https://github.com/sdlee94/Minesweeper-AI-Reinforcement-Learning)). Flags kÃ¶nnen per `--use-flags` wieder zugeschaltet werden.
- **Reward System:** Fortschrittsbasierte Skalierung (`reward_scale = max(1, width*height/100)`), starke Verluststrafe (`-12 * scale`), hoher Gewinnbonus (`+18 * scale`). Guess-Klicks erhalten einen Malus, Frontier-ZÃ¼ge Bonuspunkte.
- **Action Masking:** `get_valid_actions()` liefert boolsche Maske; `get_action_mask()` erzeugt -inf fÃ¼r ungÃ¼ltige Aktionen (wird direkt in den Q-Werten verwendet).

**Tests:** 13 TestfÃ¤lle (Initialisierung, Reset, Rewards, Masken, Flag-Rewards, usw.)

---

### 4.2 DQN Network (`src/reinforcement_learning/network.py`)

**Zweck:** CNN extrahiert rÃ¤umliche Muster und gibt Q-Werte fÃ¼r jede erlaubte Aktion zurÃ¼ck.

**Architektur (conv128x4_dense512x2):**
```
Input: (batch, 7, H, W)
â”œâ”€â”€ [Conv2d + BatchNorm + ReLU] Ã— 4   (je 128 Filter, kernel=3, padding=1)
â”œâ”€â”€ AdaptiveAvgPool2d(8 Ã— 8)          (grenzenlos fÃ¼r verschiedene BrettgrÃ¶ÃŸen)
â”œâ”€â”€ Flatten â†’ 128 Ã— 8 Ã— 8 = 8192 Features
â”œâ”€â”€ Linear(8192 â†’ 512) + ReLU + Dropout(0.25)
â”œâ”€â”€ Linear(512 â†’ 512) + ReLU + Dropout(0.25)
â””â”€â”€ Linear(512 â†’ num_actions)
```

**Reasoning:**
- 4 tiefe Conv-BlÃ¶cke entsprechen dem in [sdlee94](https://github.com/sdlee94/Minesweeper-AI-Reinforcement-Learning) erprobten Setup und verbessern die Frontier-Erkennung.
- Adaptive Pooling sorgt dafÃ¼r, dass auch 5Ã—5- oder 40Ã—25-Bretter ohne ArchitekturÃ¤nderung funktionieren.
- Dropout reduziert Overfitting auf kleinen Boards.

**Tests:** 7 TestfÃ¤lle (Initialisierung, VorwÃ¤rtspass, Gradienten, Parameteranzahl etc.)

---

### 4.3 DQN Agent (`src/reinforcement_learning/dqn_agent.py`)

**Zweck:** Double-DQN-Agent mit Experience Replay, Masking und linearem Explorations-Schedule.

**Komponenten:**
1. **ReplayBuffer:** `deque` mit max. 10k EintrÃ¤gen, speichert zusÃ¤tzlich die zulÃ¤ssigen Aktionen des Folgezustands.
2. **Q-/Target-Network:** identische Netze; Target wird alle 100 Trainingsschritte synchronisiert.
3. **Epsilon-Greedy:** Training verwendet einen linearen Scheduler (1.0 â†’ 0.03/0.05/0.10), gesteuert im Trainer.

**Hyperparameter (abhÃ¤ngig vom Brett):**
- `lr`: Basis 0.001, skaliert fÃ¼r kleinere Bretter leicht nach oben
- `gamma`: 0.95 (â‰¤600 Felder) / 0.98 (grÃ¶ÃŸer)
- `batch_size`: 32â€“96
- `loss`: SmoothL1Loss (Huber)
- `optimizer`: Adam
- `target_update`: alle 100 Steps

**Training Process:**
1. Replay-Sampling + Maskierung ungÃ¼ltiger Aktionen
2. Online-Netz liefert `argmax_a Q(s', a)` nur Ã¼ber gÃ¼ltige Aktionen
3. Target-Netz bewertet diese Aktion (Double DQN)
4. TD-Target = Reward + `gamma * Q_target`
5. Backpropagation + Gradient Clipping (`max_norm=1.0`)
6. Zielnetz-Sync alle 100 Steps
7. Epsilon wird nach jeder Episode via `LinearSchedule` gesetzt (kein Multiplikationsrauschen mehr)

**Design-Entscheidungen:**
- **Frontier-Sampling:** Auch bei Exploration werden ZÃ¼ge nahe bekannter Zahlen bevorzugt.
- **Action Masking:** `-1e9` auf ungÃ¼ltigen Aktionen sorgt dafÃ¼r, dass `argmax` nie auf bereits aufgedeckte Zellen fÃ¤llt.
- **Greedy Evaluation:** WÃ¤hrend des Trainings werden regelmÃ¤ÃŸig episodenweise TestlÃ¤ufe mit `Îµ=0` durchgefÃ¼hrt, um echte Leistung zu messen.

**Tests:** 13 TestfÃ¤lle (ReplayBuffer, Action Selection, Training Step, Save/Load, Environment-Integration)

---

## Tests

### Test-Struktur:

```
tests/
â”œâ”€â”€ minesweeper/
â”‚   â”œâ”€â”€ test_cell.py      # Cell-Klasse Tests (9 Tests)
â”‚   â”œâ”€â”€ test_board.py     # Board-Klasse Tests (7 Tests)
â”‚   â””â”€â”€ test_game.py      # Game-Klasse Tests (10 Tests)
â”œâ”€â”€ reinforcement_learning/
â”‚   â”œâ”€â”€ test_environment.py    # Environment Tests (13 Tests)
â”‚   â”œâ”€â”€ test_network.py        # DQN Network Tests (7 Tests)
â”‚   â””â”€â”€ test_dqn_agent.py      # DQN Agent Tests (13 Tests)
â””â”€â”€ run_tests.py          # Test-Runner
```

### Test-Statistik:

- **Gesamt:** 57 Tests
- **Minesweeper:** 24 Tests
- **Reinforcement Learning:** 33 Tests
- **Alle Tests:** âœ… Bestanden

### Tests ausfÃ¼hren:

```bash
python tests/run_tests.py
# oder
python -m pytest tests/
```

---

## Zusammenfassung der Design-Entscheidungen

1. **Modulare Struktur:** Klare Trennung von Spiellogik, GUI und RL
2. **Lazy Mine Placement:** Minen werden erst beim ersten Klick platziert
3. **Auto-Aufdecken:** BFS-Algorithmus fÃ¼r benutzerfreundliches Spiel
4. **State Management:** Klare Zustandsverwaltung mit Enumerationen
5. **Signal-basierte GUI:** Lose Kopplung zwischen GUI und Spiellogik
6. **RL Environment:** Gymnasium-Ã¤hnliches Interface fÃ¼r Wiederverwendbarkeit

